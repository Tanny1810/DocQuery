# ğŸ“„ DocQuery â€” RAG-Based Document Query Platform

DocQuery is an **end-to-end backend system** for **document ingestion, asynchronous processing, and Retrieval-Augmented Generation (RAG)**.  
It enables users to upload documents, generate embeddings, and query them using natural language in a scalable, production-inspired architecture.

This project is built as a **learning-focused yet real-world system**, emphasizing clean backend design, distributed processing, and modern AI-backed retrieval techniques.

---

## âœ¨ Problem Statement

Large Language Models (LLMs) cannot:
- Reliably process large documents
- Access private or user-uploaded data
- Guarantee factual answers without grounding

DocQuery solves this by:
- Chunking documents into smaller units
- Converting text into embeddings
- Retrieving only relevant chunks at query time
- Generating answers grounded in document context using RAG

---

## ğŸš€ Core Features

- ğŸ“¥ Document upload with metadata tracking
- âš™ï¸ Asynchronous document processing using background workers
- âœ‚ï¸ Text extraction and configurable chunking
- ğŸ§  Embedding generation pipeline
- ğŸ” Semantic search using vector similarity
- ğŸ“¨ Message-driven architecture using RabbitMQ
- â˜ï¸ Cloud-ready storage using S3-compatible buckets
- ğŸ“Š Explicit document lifecycle management

---

## ğŸ—ï¸ Clean Architecture Overview

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    Client    â”‚
            â”‚ (UI / API)   â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ HTTP
                â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     FastAPI API         â”‚
            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
            â”‚ â€¢ Request validation   â”‚
            â”‚ â€¢ Metadata storage     â”‚
            â”‚ â€¢ Upload to S3         â”‚
            â”‚ â€¢ Publish task         â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚         â”‚
                â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ PostgreSQL   â”‚     â”‚   RabbitMQ   â”‚
            â”‚ (Metadata)  â”‚     â”‚ (Task Queue) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚ Background Worker â”‚
                                â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                                â”‚ â€¢ Download file  â”‚
                                â”‚ â€¢ Extract text   â”‚
                                â”‚ â€¢ Chunk text     â”‚
                                â”‚ â€¢ Embed chunks   â”‚
                                â”‚ â€¢ Store vectors  â”‚
                                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚  Vector DB   â”‚
                                â”‚ (FAISS etc.) â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### High-Level Flow
1. Client uploads a document
2. FastAPI API validates request and stores metadata
3. File is uploaded to object storage (S3)
4. Task is published to RabbitMQ
5. Background worker processes the document
6. Embeddings are stored in the vector database
7. User queries retrieve relevant chunks for RAG

---

## ğŸ” Sequence Diagram â€” Document Ingestion & Processing

```mermaid
sequenceDiagram
    autonumber

    participant U as User / Client
    participant API as FastAPI API
    participant DB as PostgreSQL
    participant S3 as S3 Storage
    participant MQ as RabbitMQ
    participant W as Background Worker
    participant VDB as Vector DB (FAISS)

    U->>API: Upload document
    API->>API: Validate request
    API->>DB: Store document metadata
    API->>S3: Upload document file
    API->>MQ: Publish processing task

    MQ->>W: Consume task
    W->>S3: Download document
    W->>W: Extract text (PDF/DOCX)
    W->>W: Chunk text
    W->>W: Generate embeddings
    W->>VDB: Store vectors

    W->>DB: Update document status (PROCESSED)
---

## ğŸ§  RAG Processing Flow

            User Query
                â”‚
                â–¼
            FastAPI API
                â”‚
                â–¼
            Vector Search (Top-K chunks)
                â”‚
                â–¼
            LLM (Context + Question)
                â”‚
                â–¼
            Final Answer

---


1. Upload document
2. Extract text (PDF / DOCX)
3. Chunk text
4. Generate embeddings
5. Store vectors
6. Perform semantic search (Top-K)
7. Generate answer using LLM + retrieved context

---

## ğŸ› ï¸ Tech Stack

### Backend
- **Python 3.11**
- **FastAPI**
- **SQLAlchemy**
- **PostgreSQL**

### Async & Messaging
- **RabbitMQ**
- Dedicated background worker service

### Storage
- **AWS S3** (or any S3-compatible storage)

### Vector Search
- **FAISS** (pluggable for other vector databases)

### Infrastructure
- **Docker**
- **Docker Compose**

---

## ğŸ“ Project Structure

docquery/
â”œâ”€â”€ api/
â”‚ â”œâ”€â”€ routers/ # API endpoints
â”‚ â”œâ”€â”€ services/ # Business logic
â”‚ â”œâ”€â”€ db/ # ORM models & repositories
â”‚ â””â”€â”€ main.py
â”‚
â”œâ”€â”€ worker/
â”‚ â”œâ”€â”€ consumers/ # RabbitMQ consumers
â”‚ â”œâ”€â”€ processors/ # Extraction, chunking, embedding
â”‚ â””â”€â”€ main.py
â”‚
â”œâ”€â”€ shared/
â”‚ â”œâ”€â”€ config/ # Centralized config
â”‚ â”œâ”€â”€ constants/ # Enums & status definitions
â”‚ â””â”€â”€ utils/
â”‚
â”œâ”€â”€ docs/
â”‚ â””â”€â”€ architecture.png
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

---

## ğŸ”„ Document Lifecycle

Each document follows a clear lifecycle:

- `UPLOADED` â€” File received and stored
- `PROCESSING` â€” Worker is handling the document
- `PROCESSED` â€” Embeddings successfully stored
- `FAILED` â€” Error during processing

This makes the system **observable, debuggable, and production-ready**.

---

## âš™ï¸ Local Development Setup

```bash
git clone https://github.com/Tanny1810/DocQuery.git docquery
cd docquery
docker-compose up --build
```

---

## âš™ï¸ Services Started

- FastAPI API  
- RabbitMQ  
- PostgreSQL  
- Background Worker  

---

## ğŸ§© Key Design Decisions

- Separate worker service to isolate heavy computation  
- Message queue-based processing for reliability and scalability  
- Chunk-based embeddings to handle large documents efficiently  
- Explicit status tracking for observability  
- Cloud-compatible architecture without vendor lock-in  

---

## ğŸ¯ Learning Objectives

- Build a complete RAG system  
- Design scalable backend architectures  
- Work with message queues and workers  
- Understand vector databases and embeddings  
- Apply clean code and modular design principles  

---

## ğŸš§ Future Enhancements

- Authentication and authorization  
- API rate limiting  
- Multi-document querying  
- Streaming LLM responses  
- Support for multiple vector databases  
- Frontend UI  

---

## ğŸ“„ License

This project is intended for **educational and portfolio purposes**.

---

## ğŸ‘¤ Author

**Tanmay Chauhan**  
Backend Engineer | Python | Distributed Systems | RAG
