from app.services.storage_service import download_file
from app.processors.text_extractor import extract_text, clean_text
from app.processors.chunker import chunk_text
from shared.embeddings.embedder import embed_chunks
from app.db.vector_store import store_embeddings, get_vector_count
from app.db.document_repo import (
    update_document_status,
    get_document_storage_info,
    get_document_for_update,
    insert_chunks,
)
from app.services.document_retry_service import increment_retry_or_fail
from app.constants.document_status import DocumentStatus
from shared.config.logging import get_logger

logger = get_logger(__name__)


async def process_document(payload: dict):
    document_id = payload["document_id"]

    logger.info(f"üìÑ Processing document {document_id}")

    doc = get_document_for_update(document_id)

    if doc["status_id"] != DocumentStatus.QUEUED:
        logger.info(f"‚è≠Ô∏è Skipping document {document_id}, " f"status={doc['status_id']}")
        return

    file_path = None

    try:
        # 0Ô∏è‚É£ Mark PROCESSING
        update_document_status(document_id, DocumentStatus.PROCESSING)
        # 1Ô∏è‚É£ Fetch storage info from DB (SOURCE OF TRUTH)
        storage = get_document_storage_info(document_id)

        # 2Ô∏è‚É£ Download file
        file_path = download_file(
            provider=storage["storage_provider"],
            bucket=storage["storage_bucket"],
            key=storage["storage_key"],
        )

        # 3Ô∏è‚É£ Extract text
        text = extract_text(file_path)

        # üîß CLEAN HERE
        text = clean_text(text)

        # 4Ô∏è‚É£ Chunk text
        chunks = chunk_text(text)

        if not chunks:
            raise ValueError("No chunks generated from document")

        # 5Ô∏è‚É£ Embed
        embeddings = embed_chunks(chunks)

        # 6Ô∏è‚É£ Store embeddings
        vector_ids = store_embeddings(embeddings)

        logger.info(f"‚úÖ Stored {len(embeddings)} embeddings")
        logger.info(f"üìä Total vectors in FAISS: {get_vector_count()}")

        insert_chunks(
            document_id=document_id,
            chunks=chunks,
            vector_ids=vector_ids,
        )
        logger.info(f"‚úÖ Stored {len(chunks)} chunks")

        # 7Ô∏è‚É£ Mark READY
        update_document_status(document_id, DocumentStatus.READY)

    except Exception as exc:
        logger.exception(f"‚ùå Failed processing document {document_id}: {exc}")
        increment_retry_or_fail(document_id, exc)
        return

    finally:
        # 8Ô∏è‚É£ Cleanup temp file
        if file_path:
            try:
                logger.info(f"üßπ Cleaning up temp file {file_path}")
                file_path.unlink(missing_ok=True)
            except Exception:
                logger.warning(f"‚ö†Ô∏è Failed to cleanup temp file {file_path}")
